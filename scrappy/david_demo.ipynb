{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo is a collaboration with Nicolas (x-machina), made for David from https://www.intuitecht.com/\n",
    "For more details, see: https://docs.google.com/document/d/1hm5J3sREaap3G5HcK1QK50tyBTDkmtpn6TXhZ2mxmo4/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from vertexai.preview.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Tool,\n",
    ")\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\", \"atbv-sb-datascience\")\n",
    "LOCATION_ID = os.environ.get(\"LOCATION_ID\", \"northamerica-northeast1\")\n",
    "DEFAULT_MODEL = os.environ.get(\"DEFAULT_MODEL\", \"gemini-1.5-pro-001\")\n",
    "GOOGLE_SEARCH_API = os.environ.get(\"GOOGLE_SEARCH_API\")\n",
    "GOOGLE_CSE_ID = os.environ.get(\"GOOGLE_CSE_ID\")\n",
    "LOGGING_MODE = os.environ.get(\"LOGGING_MODE\", \"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(filepath):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000, chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    loader = PyPDFLoader(filepath, extract_images=False)\n",
    "    doc_pdf = loader.load()\n",
    "    documents = text_splitter.split_documents(doc_pdf)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = load_pdf('hydraulic.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_docs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(question:str, docs:List[Document]):\n",
    "\n",
    "    prompt = f\"\"\"You are an expert scientist that processes questions about hydraulic pumps and returns an expert answer. \n",
    "    You are tasked with with giving an answer to the following question: \n",
    "    {question}.\n",
    "    You are given the following documents to use as a reference.\n",
    "    Your answer should ONLY rely on these sources and not on any prior knowledge.\n",
    "    Here are the documents:\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in pdf_docs:\n",
    "        prompt += \"\"\"\n",
    "        page: {doc['page']}\n",
    "        content: {doc['page_content']}\n",
    "        \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: dict\n",
    "    # instruction: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question_func = FunctionDeclaration(\n",
    "    name=\"answerQuestion\",\n",
    "    description=\"Answers a question with references to relevant pages from a source document\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"page\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The relevant page in the source document\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question asked\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"title\", \"summary\", \"source\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = Tool(function_declarations=[answer_question_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(question='sdfsdfdsf', docs=pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.5-pro-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = model.generate_content(\n",
    "    prompt,\n",
    "    tools=[tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOING IT WITH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG PORTION\n",
    "\n",
    "def get_retriever(\n",
    "    documents,\n",
    "    embedding,\n",
    "    session,\n",
    "    retriver_db,\n",
    "    search_topk=15,\n",
    "    supabase_url=None,\n",
    "    supabase_key=None,\n",
    "    supabase_chunksize=500,\n",
    "):\n",
    "    if retriver_db != \"SUPABASE\":\n",
    "        ## Joseph 20240506: Change default vector storage to firestore\n",
    "        # try:\n",
    "        #     contracts_vector_db = Chroma.from_documents(collection_name=\"documents\", documents=documents, embedding=embedding)\n",
    "        # except:\n",
    "        #     contracts_vector_db = Chroma(collection_name=\"documents\", embedding_function=embedding)\n",
    "        try:\n",
    "            ## TODO: Need create index before deployment:\n",
    "            # gcloud alpha firestore indexes composite create \\\n",
    "            # --project=atbv-sb-datascience \\\n",
    "            # --collection-group=ai_studio_documents \\\n",
    "            # --query-scope=COLLECTION \\\n",
    "            # --field-config field-path=embedding,vector-config='{\"dimension\":\"768\", \"flat\": \"{}\"}' \\\n",
    "            # --database=test\n",
    "            contracts_vector_db = FirestoreVectorStore.from_documents(\n",
    "                client=FIRESTORE_CLIENT,\n",
    "                collection=\"ai_studio_documents\",\n",
    "                documents=documents,\n",
    "                embedding=embedding,\n",
    "                filters=FieldFilter(\"metadata.session_id\", \"==\", f\"{session}\")\n",
    "            )\n",
    "        except:\n",
    "            contracts_vector_db = FirestoreVectorStore(\n",
    "                client=FIRESTORE_CLIENT,\n",
    "                collection=\"ai_studio_documents\", \n",
    "                embedding=embedding,\n",
    "                filters=FieldFilter(\"metadata.session_id\", \"==\", f\"{session}\")\n",
    "            )\n",
    "    else:\n",
    "        try:\n",
    "            supabase: Client = create_client(supabase_url, supabase_key)\n",
    "            contracts_vector_db = SupabaseVectorStore.from_documents(\n",
    "                documents,\n",
    "                embedding,\n",
    "                client=supabase,\n",
    "                table_name=\"documents\",\n",
    "                query_name=\"match_documents\",\n",
    "                chunk_size=supabase_chunksize,\n",
    "            )\n",
    "        except:\n",
    "            contracts_vector_db = SupabaseVectorStore(\n",
    "                client=supabase,\n",
    "                embedding=embedding,\n",
    "                table_name=\"documents\",\n",
    "                query_name=\"match_documents\",\n",
    "            )\n",
    "        \n",
    "\n",
    "    return contracts_vector_db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": search_topk,\n",
    "            \"filter\": {\"session_id\": session},\n",
    "        },  # Filter search result by session id\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def embed_retriver(\n",
    "    documents,\n",
    "    retriver_db,\n",
    "    session,\n",
    "    supabase_url=None,\n",
    "    supabase_key=None,\n",
    "    embedding_model=\"textembedding-gecko@latest\",  # The API accepts a maximum of 3,072 input tokens and outputs 768-dimensional vector embeddings. Use the following parameters for the text embeddings model textembedding-gecko,\n",
    "    search_topk=15,\n",
    "):\n",
    "    try:\n",
    "        embedding = VertexAIEmbeddings(model_name=embedding_model)\n",
    "        # contracts_vector_db = Chroma.from_documents(documents, embedding)\n",
    "        # retriever = contracts_vector_db.as_retriever(\n",
    "        #     search_type=\"similarity\", search_kwargs={\"k\": search_topk}\n",
    "        # )\n",
    "        retriever = get_retriever(\n",
    "            documents,\n",
    "            embedding,\n",
    "            session,\n",
    "            retriver_db,\n",
    "            search_topk,\n",
    "            supabase_url,\n",
    "            supabase_key,\n",
    "        )\n",
    "        return retriever\n",
    "    except Exception as combine_documents_chain_err:\n",
    "        raise combine_documents_chain_err\n",
    "    \n",
    "    \n",
    "def invoke_retriver(llm, retriever, question):\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", prompt_template.contextualize_q_system_prompt),\n",
    "                    MessagesPlaceholder(\"chat_history\"),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt_template.qa_system_template),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    ##Joseph 20240521: Chat history not being populated in this version to minimize hallucination.\n",
    "    chat_history = []\n",
    "    \n",
    "    res_answer = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "    \n",
    "    return res_answer\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7d7b4a72a7ab1202c1aa1d2d4ccffc9579def15d15f416b3055c17297e196fd"
  },
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
